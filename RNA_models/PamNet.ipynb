{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0294fd41-9c86-473b-abbe-d7a1eff02a3d",
   "metadata": {},
   "source": [
    "# PaMNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab00cdf-5d78-405a-8fed-57ce16c7a52f",
   "metadata": {},
   "source": [
    "Github: https://github.com/XieResearchGroup/Physics-aware-Multiplex-GNN\n",
    "\n",
    "Place this file in the PaMNet repository, alongside the other primary files.\n",
    "\n",
    "```bash\n",
    "env_name=PaMNet\n",
    "conda env list | cut -d' ' -f1 | grep -q ${env_name} && echo \"${env_name} already installed\" || conda create --name ${env_name} python=3.10 -y\n",
    "conda activate ${env_name}\n",
    "# install packages here?\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da453e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and install?\n",
    "!git clone https://github.com/XieResearchGroup/Physics-aware-Multiplex-GNN\n",
    "!cd Physics-aware-Multiplex-GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a16941-35f7-49fa-bc4f-07731630452f",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "971d1e10-4e89-4b6f-8c3f-0bc731e58146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing example_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 14000/14000 [12:08<00:00, 19.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing example_val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [03:16<00:00, 20.34it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from rdkit import Chem\n",
    "\n",
    "\n",
    "def load_molecule(molecule_file):\n",
    "    if \".mol2\" in molecule_file:\n",
    "        my_mol = Chem.MolFromMol2File(molecule_file, sanitize=False, removeHs=True)\n",
    "    elif \".sdf\" in molecule_file:\n",
    "        suppl = Chem.SDMolSupplier(str(molecule_file), sanitize=False, removeHs=True)\n",
    "        my_mol = suppl[0]\n",
    "    elif \".pdb\" in molecule_file:\n",
    "        my_mol = Chem.MolFromPDBFile(\n",
    "            str(molecule_file), sanitize=False, removeHs=True)\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized file type for %s\" % str(molecule_file))\n",
    "    if my_mol is None:\n",
    "        raise ValueError(\"Unable to read non None Molecule Object\")\n",
    "    xyz = get_xyz_from_mol(my_mol)\n",
    "    return xyz, my_mol\n",
    "\n",
    "def get_xyz_from_mol(mol):\n",
    "    xyz = np.zeros((mol.GetNumAtoms(), 3))\n",
    "    conf = mol.GetConformer()\n",
    "    for i in range(conf.GetNumAtoms()):\n",
    "        position = conf.GetAtomPosition(i)\n",
    "        xyz[i, 0] = position.x\n",
    "        xyz[i, 1] = position.y\n",
    "        xyz[i, 2] = position.z\n",
    "    return (xyz)\n",
    "\n",
    "def get_rms(molecule_file):\n",
    "    with open(molecule_file) as f:\n",
    "        for line in f:\n",
    "            if 'TER' in line:\n",
    "                break\n",
    "        for line in f:\n",
    "            cont = line.split()\n",
    "            if cont[0] == 'rms':\n",
    "                break\n",
    "    return float(cont[-1])\n",
    "\n",
    "def construct_graphs(data_dir, save_dir, data_name, save_name):\n",
    "    print(\"Preprocessing\", data_name)\n",
    "\n",
    "    data_dir_full = os.path.join(data_dir, data_name)\n",
    "    save_dir_full = os.path.join(save_dir, save_name, \"raw\")\n",
    "\n",
    "    if not os.path.exists(save_dir_full):\n",
    "        os.makedirs(save_dir_full)\n",
    "       \n",
    "    name_list = [x for x in os.listdir(data_dir_full)]\n",
    "\n",
    "    for file_name in [save_name + '_node_labels.txt', save_name + '_graph_indicator.txt', \n",
    "                save_name + '_node_attributes.txt', save_name + '_graph_labels.txt',\n",
    "                save_name + '_graph_names.txt']:\n",
    "        if os.path.isfile(os.path.join(save_dir_full, file_name)):\n",
    "            os.remove(os.path.join(save_dir_full, file_name))\n",
    "\n",
    "    for i in tqdm(range(len(name_list))):\n",
    "        name = name_list[i]\n",
    "        rna_file = os.path.join(data_dir_full, name)\n",
    "        \n",
    "        rna_coords, rna_mol = load_molecule(rna_file)\n",
    "        rna_label = get_rms(rna_file)\n",
    "\n",
    "        rna_x = list()\n",
    "        for atom_id in rna_mol.GetAtoms():\n",
    "            rna_x.append(atom_id.GetAtomicNum())\n",
    "\n",
    "        x_indices = [i for i,x in enumerate(rna_x) if (x == 6 or x == 7 or x == 8)] \n",
    "        rna_x = np.array([rna_x[i] for i in x_indices])\n",
    "        rna_pos = np.array(rna_coords[x_indices])\n",
    "\n",
    "        types = {\n",
    "            6: 0,   #C\n",
    "            7: 1,   #N\n",
    "            8: 2,   #O\n",
    "        }\n",
    "\n",
    "        rna_x = np.array([types[x] for x in rna_x])\n",
    "\n",
    "        name = np.array(name).reshape(-1, 1)\n",
    "\n",
    "        # Generate files for loading graphs\n",
    "        indicator = np.ones((rna_x.shape[0], 1)) * (i + 1)\n",
    "\n",
    "        with open(os.path.join(save_dir_full, save_name + '_graph_indicator.txt'),'ab') as f:\n",
    "            np.savetxt(f, indicator, fmt='%i', delimiter=', ')\n",
    "        f.close()\n",
    "    \n",
    "        with open(os.path.join(save_dir_full, save_name + '_node_labels.txt'),'ab') as f:\n",
    "            np.savetxt(f, rna_x, fmt='%i', delimiter=', ')\n",
    "        f.close()\n",
    "  \n",
    "        with open(os.path.join(save_dir_full, save_name + '_node_attributes.txt'),'ab') as f:\n",
    "            np.savetxt(f, rna_pos, fmt='%.3f', delimiter=', ')\n",
    "        f.close()\n",
    "        \n",
    "        with open(os.path.join(save_dir_full, save_name + '_graph_labels.txt'),'ab') as f:\n",
    "            np.savetxt(f, [rna_label], fmt='%.3f', delimiter=', ')\n",
    "        f.close()\n",
    "\n",
    "        with open(os.path.join(save_dir_full, save_name + '_graph_names.txt'),'ab') as f:\n",
    "            np.savetxt(f, name, fmt='%s', delimiter=', ')\n",
    "        f.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    data_dir = os.path.join(\".\", \"data\", \"RNA-Puzzles\", \"classics_train_val\")\n",
    "    save_dir = os.path.join(\".\", \"data\", \"RNA-Puzzles\")\n",
    "\n",
    "    construct_graphs(data_dir, save_dir, \"example_train\", \"train\")\n",
    "    construct_graphs(data_dir, save_dir, \"example_val\", \"val\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0270f6b9-28df-4d96-8d7f-eabe84500448",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3001e37-d509-4f76-8942-b5520d949b56",
   "metadata": {},
   "source": [
    "Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f909ce2-9012-4ac6-9979-aa5718a5bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    gpu=0,\n",
    "    seed=40,\n",
    "    dataset='RNA-Puzzles',\n",
    "    epochs=15,\n",
    "    lr=1e-4,\n",
    "    wd=0,\n",
    "    n_layer=1,\n",
    "    dim=16,\n",
    "    batch_size=8,\n",
    "    cutoff_l=2.6,\n",
    "    cutoff_g=20.0,\n",
    "    flow='target_to_source'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b62fac-c573-43e0-a919-f400d4fee87e",
   "metadata": {},
   "source": [
    "Change numpy.math to math in sbf.py before this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2610f199-c1c6-4b88-b7a7-66703756aba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\Desktop\\RNA\\Physics-aware-Multiplex-GNN\\datasets\\tu_dataset.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n",
      "C:\\Users\\nikhi\\anaconda3\\envs\\RNA\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\nikhi\\anaconda3\\envs\\RNA\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n",
      "Start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\Desktop\\RNA\\Physics-aware-Multiplex-GNN\\models.py:167: UserWarning: Using torch.cross without specifying the dim arg is deprecated.\n",
      "Please either pass the dim explicitly or simply use torch.linalg.cross.\n",
      "The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Cross.cpp:66.)\n",
      "  b = torch.cross(pos_ji, pos_kj).norm(dim=-1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 111\u001b[0m\n\u001b[0;32m     95\u001b[0m args \u001b[38;5;241m=\u001b[39m argparse\u001b[38;5;241m.\u001b[39mNamespace(\n\u001b[0;32m     96\u001b[0m     gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     97\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m     flow\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_to_source\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    108\u001b[0m )\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 82\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     79\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     81\u001b[0m train_loss, _ \u001b[38;5;241m=\u001b[39m test(model, train_loader, device)\n\u001b[1;32m---> 82\u001b[0m val_loss, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{:03d}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{:.7f}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{:.7f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, train_loss, val_loss))\n\u001b[0;32m     86\u001b[0m save_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 31\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(model, loader, device)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     30\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 31\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     pred_list \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     33\u001b[0m     y_list \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RNA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RNA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\Desktop\\RNA\\Physics-aware-Multiplex-GNN\\models.py:143\u001b[0m, in \u001b[0;36mPAMNet.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    140\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mindex_select(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings, \u001b[38;5;241m0\u001b[39m, x_raw[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m    141\u001b[0m pos \u001b[38;5;241m=\u001b[39m x_raw[:,:\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m--> 143\u001b[0m row, col \u001b[38;5;241m=\u001b[39m \u001b[43mknn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m edge_index_knn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([row, col], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    145\u001b[0m edge_index_knn, dist_knn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_edge_info(edge_index_knn, pos)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RNA\\lib\\site-packages\\torch_geometric\\nn\\pool\\__init__.py:117\u001b[0m, in \u001b[0;36mknn\u001b[1;34m(x, y, k, batch_x, batch_y, cosine, num_workers, batch_size)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch_geometric\u001b[38;5;241m.\u001b[39mtyping\u001b[38;5;241m.\u001b[39mWITH_TORCH_CLUSTER_BATCH_SIZE:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_cluster\u001b[38;5;241m.\u001b[39mknn(x, y, k, batch_x, batch_y, cosine,\n\u001b[0;32m    116\u001b[0m                              num_workers)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_cluster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcosine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RNA\\lib\\site-packages\\torch_cluster\\knn.py:81\u001b[0m, in \u001b[0;36mknn\u001b[1;34m(x, y, k, batch_x, batch_y, cosine, num_workers, batch_size)\u001b[0m\n\u001b[0;32m     78\u001b[0m     ptr_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbucketize(arange, batch_x)\n\u001b[0;32m     79\u001b[0m     ptr_y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbucketize(arange, batch_y)\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_cluster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcosine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RNA\\lib\\site-packages\\torch\\_ops.py:1116\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[1;32m-> 1116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "from models import PAMNet, Config\n",
    "from datasets import TUDataset\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    pred_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data)\n",
    "        pred_list += pred.reshape(-1).tolist()\n",
    "        y_list += data.y.reshape(-1).tolist()\n",
    "\n",
    "    pred = np.array(pred_list).reshape(-1,)\n",
    "    pred = torch.tensor(pred).to(device)\n",
    "\n",
    "    y = np.array(y_list).reshape(-1,)\n",
    "    y = torch.tensor(y).to(device)\n",
    "\n",
    "    loss = F.smooth_l1_loss(pred, y)\n",
    "    return loss.item(), np.array(pred_list).reshape(-1,)\n",
    "\n",
    "def main(args):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "    \n",
    "    set_seed(args.seed)\n",
    "\n",
    "    # Create dataset\n",
    "    path = osp.join('.', 'data', args.dataset)\n",
    "    train_dataset = TUDataset(path, name='train', use_node_attr=True).shuffle()\n",
    "    val_dataset = TUDataset(path, name='val', use_node_attr=True)\n",
    "\n",
    "    # Load dataset\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    print(\"Data loaded!\")\n",
    "\n",
    "    config = Config(dataset=args.dataset, dim=args.dim, n_layer=args.n_layer, cutoff_l=args.cutoff_l, \n",
    "                    cutoff_g=args.cutoff_g, flow=args.flow)\n",
    "\n",
    "    model = PAMNet(config).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd, amsgrad=False)\n",
    "    \n",
    "    print(\"Start training!\")\n",
    "    best_val_loss = None\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(data)\n",
    "            loss = F.smooth_l1_loss(output, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss, _ = test(model, train_loader, device)\n",
    "        val_loss, _ = test(model, val_loader, device)\n",
    "\n",
    "        print('Epoch: {:03d}, Train Loss: {:.7f}, Val Loss: {:.7f}'.format(epoch+1, train_loss, val_loss))\n",
    "        \n",
    "        save_folder = os.path.join(\".\", \"save\")\n",
    "        if not os.path.exists(save_folder):\n",
    "            os.makedirs(save_folder)\n",
    "\n",
    "        if best_val_loss is None or val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(save_folder, \"pamnet_rna_best.pt\"))\n",
    "\n",
    "# Create args namespace with your specified parameters\n",
    "args = argparse.Namespace(\n",
    "    gpu=0,\n",
    "    seed=40,\n",
    "    dataset='RNA-Puzzles',\n",
    "    epochs=15,\n",
    "    lr=1e-4,\n",
    "    wd=0,\n",
    "    n_layer=1,\n",
    "    dim=16,\n",
    "    batch_size=8,\n",
    "    cutoff_l=2.6,\n",
    "    cutoff_g=20.0,\n",
    "    flow='target_to_source'\n",
    ")\n",
    "\n",
    "# Run the main function\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94684e9d-038a-43ee-8504-8e8189665bf4",
   "metadata": {},
   "source": [
    "The code execution was interrupted due to exceeding the allowed time limit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f37329-4d0e-4f46-ac48-9f4cf82f6cc7",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "375315b3-7fef-4004-8296-b807081d20d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\Desktop\\RNA\\Physics-aware-Multiplex-GNN\\datasets\\tu_dataset.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n",
      "C:\\Users\\nikhi\\anaconda3\\envs\\RNA\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\nikhi\\anaconda3\\envs\\RNA\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\AppData\\Local\\Temp\\ipykernel_31132\\3002995870.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"./save/\" + args.saved_model, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. Start prediction!\n",
      "\n",
      "Predictions:\n",
      "==================================================\n",
      "Structure Name                 Prediction\n",
      "--------------------------------------------------\n",
      "rna_puzzle_20_NATIVE_5y87_RNA  2.6660\n",
      "rna_puzzle_14_free_NATIVE_14_5ddo_free_solution_rpr 3.6808\n",
      "rna_puzzle_14_bound_NATIVE_14_5ddp_bound_solution_rpr 3.6472\n",
      "rna_puzzle_10_NATIVE_10_0_solution_4LCK_rpr 3.2907\n",
      "rna_puzzle_1_NATIVE_1_solution_0_rpr 2.2482\n",
      "rna_puzzle_7_NATIVE_7_0_solution_4r4v_rpr 3.3068\n",
      "rna_puzzle_21_NATIVE_21_5nwq_solution_0_rpr 2.9127\n",
      "rna_puzzle_6_NATIVE_6_0_solution_4GXY_rpr 3.5250\n",
      "rna_puzzle_19_NATIVE_19_5t5a_solution_0_rpr 2.6696\n",
      "rna_puzzle_15_NATIVE_15_solution_0_rpr 3.2712\n",
      "rna_puzzle_12_NATIVE_12_4qln_solution_rpr 2.9578\n",
      "rna_puzzle_17_NATIVE_17_5k7c_solution_rpr 3.2965\n",
      "rna_puzzle_4_with_3IQP_NATIVE_4_0_solution_3V7E_rpr 3.3117\n",
      "rna_puzzle_2_NATIVE_hacked_on  3.4877\n",
      "rna_puzzle_18_with_4PQV_NATIVE_18_0_solution_5TPY_rpr 3.1077\n",
      "rna_puzzle_9_2xnw_NATIVE_5kpy  3.2752\n",
      "rna_puzzle_3_NATIVE_3_solution_0_rpr 2.8389\n",
      "rna_puzzle_11_NATIVE           2.8011\n",
      "rna_puzzle_5_homology_NATIVE_5_0_solution_4p8z_rpr 3.0770\n",
      "rna_puzzle_8_NATIVE_8_0_solution_4L81_rpr 3.4548\n",
      "rna_puzzle_13_NATIVE_13_0_solution_4XW7_rpr 3.2016\n",
      "==================================================\n",
      "\n",
      "Summary Statistics:\n",
      "Mean prediction: 3.1442\n",
      "Min prediction: 2.2482\n",
      "Max prediction: 3.6808\n",
      "Total structures evaluated: 21\n",
      "\n",
      "Prediction saved to: .\\rna_puzzles_predictions\\rna_native.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "from models import PAMNet, Config\n",
    "from datasets import TUDataset\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def main(args):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "    \n",
    "    set_seed(args.seed)\n",
    "\n",
    "    # Create dataset\n",
    "    path = osp.join('.', 'data', 'RNA-Puzzles')\n",
    "    test_dataset = TUDataset(path, name=args.dataset, use_node_attr=True)\n",
    "\n",
    "    # Load dataset\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    print(\"Data loaded!\")\n",
    "\n",
    "    config = Config(dataset=args.dataset, dim=args.dim, n_layer=args.n_layer, cutoff_l=args.cutoff_l, \n",
    "                    cutoff_g=args.cutoff_g, flow=args.flow)\n",
    "\n",
    "    model = PAMNet(config).to(device)\n",
    "    model.load_state_dict(torch.load(\"./save/\" + args.saved_model, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"Model loaded. Start prediction!\")\n",
    "    y_hat_list = []\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        output = model(data)\n",
    "        y_hat_list += output.reshape(-1).tolist()\n",
    "\n",
    "    y_hat = np.array(y_hat_list).reshape(-1,)\n",
    "\n",
    "    name_list = np.loadtxt(osp.join('.', 'data', 'RNA-Puzzles', args.dataset, 'raw', args.dataset + '_graph_names.txt'), dtype=str, converters = {0: lambda s: s[:-4]})\n",
    "\n",
    "    df['PAMNet'] = y_hat\n",
    "    df['tag'] = name_list\n",
    "    df['puzzle_number'] = args.dataset[5:]\n",
    "\n",
    "    # Print predictions with corresponding names\n",
    "    print(\"\\nPredictions:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"{'Structure Name':<30} {'Prediction':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    for idx, row in df.iterrows():\n",
    "        print(f\"{row['tag']:<30} {row['PAMNet']:>.4f}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(f\"Mean prediction: {df['PAMNet'].mean():.4f}\")\n",
    "    print(f\"Min prediction: {df['PAMNet'].min():.4f}\")\n",
    "    print(f\"Max prediction: {df['PAMNet'].max():.4f}\")\n",
    "    print(f\"Total structures evaluated: {len(df)}\")\n",
    "\n",
    "    if not os.path.exists(osp.join('.', 'rna_puzzles_predictions')):\n",
    "        os.makedirs(osp.join('.', 'rna_puzzles_predictions'))\n",
    "\n",
    "    file_name = osp.join('.', 'rna_puzzles_predictions', args.dataset + '.csv')\n",
    "    df.to_csv(file_name, sep=',', index=False)\n",
    "    \n",
    "    print(\"\\nPrediction saved to:\", file_name)\n",
    "    \n",
    "    return df  # Return the dataframe for further analysis if needed\n",
    "\n",
    "# Create args namespace with your specified parameters\n",
    "args = argparse.Namespace(\n",
    "    gpu=0,\n",
    "    seed=40,\n",
    "    dataset='rna_native',\n",
    "    epochs=150,\n",
    "    lr=1e-4,\n",
    "    wd=0,\n",
    "    n_layer=1,\n",
    "    dim=16,\n",
    "    batch_size=16,\n",
    "    cutoff_l=2.6,\n",
    "    cutoff_g=20.0,\n",
    "    flow='target_to_source',\n",
    "    saved_model='pamnet_rna.pt'\n",
    ")\n",
    "\n",
    "# Run the main function and store the results\n",
    "predictions_df = main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2148748d-23f4-40fa-be62-44bc038255d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
