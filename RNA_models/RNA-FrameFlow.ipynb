{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b069cbe2-c645-46e7-bfd9-9d7411ac816e",
   "metadata": {},
   "source": [
    "## RNA-FrameFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b7921c-7606-4fda-9357-17c027a90939",
   "metadata": {},
   "source": [
    "!git clone https://github.com/rish-16/rna-backbone-design.git\n",
    "!cd rna-backbone-design/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af48e6a-3267-472b-b230-5238878ad3f5",
   "metadata": {},
   "source": [
    "```bash\n",
    "env_name=\"rna-bb-design\"\n",
    "\n",
    "\n",
    "conda env list | cut -d' ' -f1 | grep -q \"^${env_name}$\" \\\n",
    "  && echo \"${env_name} environment already exists\" \\\n",
    "  || conda create -n ${env_name} python=3.10 -y\n",
    "\n",
    "\n",
    "conda activate ${env_name}\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9bb8d15-f3ee-4f29-bafc-200ea640d435",
   "metadata": {},
   "source": [
    "chmod +x master_rna_install.sh\n",
    "bash master_rna_install.sh"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73090359-93ce-4bdd-a755-1ed9674fc1aa",
   "metadata": {},
   "source": [
    "# create data directory\n",
    "mkdir data/rnasolo\n",
    "cd data/rnasolo\n",
    "\n",
    "# download RNAsolo\n",
    "wget https://rnasolo.cs.put.poznan.pl/media/files/zipped/bunches/pdb/all_member_pdb_4_0__3_326.zip \n",
    "unzip all_member_pdb_4_0__3_326.zip # unzips all PDB files\n",
    "mv all_member_pdb_4_0__3_326.zip ../ # moves ZIP archive out of new file directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ab92a6-6d78-4e25-b79f-2bab68e5b233",
   "metadata": {},
   "source": [
    "Revised all codes to function exclusively on CPUs.\n",
    "\n",
    "Changes done in \"flow_module.py, all_atom.py, evalsuite.py\" to work only on \"CPU\". Original Code works fine on \"GPU\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0ff3a0-e672-4b50-9a2b-934451785b45",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8eed88ef-d527-46ef-b80b-518daa889263",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce07509-066b-4a71-99cf-2e4babf55d02",
   "metadata": {},
   "source": [
    "### process_rna_pdb_files.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2406478-11c0-43d5-8980-89ee52d0beb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files will be written to ./data/rnasolo_proc/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import collections\n",
    "import functools as fn\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import mdtraj as md\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from Bio import PDB\n",
    "\n",
    "from src.data import utils\n",
    "from src.data import parsers\n",
    "\n",
    "# Fixed arguments that were previously command line parameters\n",
    "CONFIG = {\n",
    "    \"pdb_dir\": \"./data/rnasolo/\",\n",
    "    \"num_processes\": 16,\n",
    "    \"write_dir\": \"./data/rnasolo_proc/\",\n",
    "    \"skip_existing\": False,\n",
    "    \"debug\": False,\n",
    "    \"verbose\": False\n",
    "}\n",
    "\n",
    "def process_file(\n",
    "    file_path: str,\n",
    "    write_dir: str,\n",
    "    inter_chain_interact_dist_threshold: float = 7.0,\n",
    "    skip_existing: bool = False,\n",
    "    verbose: bool = False,\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Processes protein file into usable, smaller pickles.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to file to read.\n",
    "        write_dir: Directory to write pickles to.\n",
    "        inter_chain_interact_dist_threshold: Euclidean distance under which\n",
    "            to classify a pairwise inter-chain residue-atom distance as an interaction.\n",
    "        skip_existing: Whether to skip processed files.\n",
    "        verbose: Whether to log everything.\n",
    "\n",
    "    Returns:\n",
    "        Saves processed protein to pickle and returns metadata.\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    pdb_name = os.path.basename(file_path).replace(\".pdb\", \"\")\n",
    "    metadata[\"pdb_name\"] = pdb_name\n",
    "\n",
    "    pdb_subdir = os.path.join(write_dir, pdb_name[1:3].lower())\n",
    "    os.makedirs(pdb_subdir, exist_ok=True)\n",
    "    processed_path = os.path.join(pdb_subdir, f\"{pdb_name}.pkl\")\n",
    "    metadata[\"processed_path\"] = os.path.abspath(processed_path)\n",
    "    metadata[\"raw_path\"] = file_path\n",
    "    if skip_existing and os.path.exists(metadata[\"processed_path\"]):\n",
    "        return None\n",
    "    parser = PDB.PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(pdb_name, file_path)\n",
    "\n",
    "    # Extract all chains\n",
    "    struct_chains = {chain.id.upper(): chain for chain in structure.get_chains()}\n",
    "    metadata[\"num_chains\"] = len(struct_chains)\n",
    "\n",
    "    # Extract features\n",
    "    all_seqs = set()\n",
    "    struct_feats = []\n",
    "    num_na_chains = 0\n",
    "    \n",
    "    na_natype, chain_dict = None, None\n",
    "    \n",
    "    for chain_id, chain in struct_chains.items():\n",
    "        # Convert chain id into int\n",
    "        chain_index = utils.chain_str_to_int(chain_id)\n",
    "        chain_mol = parsers.process_chain_pdb(chain, chain_index, chain_id, verbose=verbose)\n",
    "        \n",
    "        if chain_mol is None:\n",
    "            # Note: Indicates that neither a protein chain nor a nucleic acid chain was found\n",
    "            continue\n",
    "        elif chain_mol[-1][\"molecule_type\"] == \"na\":\n",
    "            num_na_chains += 1\n",
    "            na_natype = (\n",
    "                chain_mol[-2]\n",
    "                if na_natype is None\n",
    "                else torch.cat((na_natype, chain_mol[-2]), dim=0)\n",
    "            )\n",
    "\n",
    "        chain_mol_constants = chain_mol[-1][\"molecule_constants\"]\n",
    "        chain_mol_backbone_atom_name = chain_mol[-1][\"molecule_backbone_atom_name\"]\n",
    "        chain_dict = parsers.macromolecule_outputs_to_dict(chain_mol)\n",
    "        \n",
    "        chain_dict = utils.parse_chain_feats_pdb(\n",
    "            chain_feats=chain_dict,\n",
    "            molecule_constants=chain_mol_constants,\n",
    "            molecule_backbone_atom_name=chain_mol_backbone_atom_name,\n",
    "        )\n",
    "        all_seqs.add(tuple(chain_dict[\"aatype\"]))\n",
    "        struct_feats.append(chain_dict)\n",
    "    \n",
    "    if chain_dict is None:\n",
    "        if verbose:\n",
    "            print(f\"No chains were found for PDB {file_path}. Skipping...\")\n",
    "        return None\n",
    "    \n",
    "    if len(all_seqs) == 1:\n",
    "        metadata[\"quaternary_category\"] = \"homomer\"\n",
    "    else:\n",
    "        metadata[\"quaternary_category\"] = \"heteromer\"\n",
    "\n",
    "    # Add assembly features\n",
    "    seq_to_entity_id = {}\n",
    "    grouped_chains = collections.defaultdict(list)\n",
    "    for chain_dict in struct_feats:\n",
    "        seq = tuple(chain_dict[\"aatype\"])\n",
    "        if seq not in seq_to_entity_id:\n",
    "            seq_to_entity_id[seq] = len(seq_to_entity_id) + 1\n",
    "        grouped_chains[seq_to_entity_id[seq]].append(chain_dict)\n",
    "\n",
    "    new_all_chain_dict = {}\n",
    "    chain_id = 1\n",
    "    for entity_id, group_chain_features in grouped_chains.items():\n",
    "        for sym_id, chain_dict in enumerate(group_chain_features, start=1):\n",
    "            new_all_chain_dict[f\"{utils.int_id_to_str_id(entity_id)}_{sym_id}\"] = chain_dict\n",
    "            seq_length = len(chain_dict[\"aatype\"])\n",
    "            chain_dict[\"asym_id\"] = chain_id * np.ones(seq_length)\n",
    "            chain_dict[\"sym_id\"] = sym_id * np.ones(seq_length)\n",
    "            chain_dict[\"entity_id\"] = entity_id * np.ones(seq_length)\n",
    "            chain_id += 1\n",
    "\n",
    "    # Concatenate features\n",
    "    complex_feats = utils.concat_np_features(struct_feats, add_batch_dim=False)\n",
    "    if complex_feats[\"bb_mask\"].sum() < 1.0:\n",
    "        return None\n",
    "    assert len(complex_feats[\"bb_mask\"]) == len(\n",
    "        complex_feats[\"aatype\"]\n",
    "    ), \"Number of core atoms must match number of residues.\"\n",
    "\n",
    "    # Record molecule metadata\n",
    "    metadata[\"num_protein_chains\"] = 0\n",
    "    metadata[\"num_na_chains\"] = num_na_chains\n",
    "\n",
    "    # Process geometry features\n",
    "    complex_aatype = complex_feats[\"aatype\"]\n",
    "    metadata[\"seq_len\"] = len(complex_aatype)\n",
    "    metadata[\"na_seq_len\"] = 0 if na_natype is None else len(na_natype)\n",
    "    modeled_idx = np.where((complex_aatype != 20) & (complex_aatype != 26))[0]\n",
    "    na_modeled_idx = None if na_natype is None else np.where(na_natype != 26)[0]\n",
    "    if np.sum((complex_aatype != 20) & (complex_aatype != 26)) == 0:\n",
    "        raise utils.LengthError(\"No modeled residues\")\n",
    "    metadata[\"modeled_seq_len\"] = np.max(modeled_idx) - np.min(modeled_idx) + 1\n",
    "    metadata[\"modeled_protein_seq_len\"] = 0\n",
    "    metadata[\"modeled_na_seq_len\"] = (\n",
    "        0 if na_natype is None else np.max(na_modeled_idx) - np.min(na_modeled_idx) + 1\n",
    "    )\n",
    "    complex_feats[\"modeled_idx\"] = modeled_idx\n",
    "    complex_feats[\"na_modeled_idx\"] = na_modeled_idx\n",
    "\n",
    "    # Find inter-chain interface residues\n",
    "    num_atoms_per_res = complex_feats[\"atom_positions\"].shape[1]\n",
    "    bb_pos = torch.from_numpy(complex_feats[\"bb_positions\"]).unsqueeze(0)\n",
    "    atom_pos = (\n",
    "        torch.from_numpy(complex_feats[\"atom_positions\"])\n",
    "        .unsqueeze(0)\n",
    "        .flatten(start_dim=1, end_dim=2)\n",
    "    )\n",
    "    bb_asym_id = torch.from_numpy(complex_feats[\"asym_id\"]).unsqueeze(0)\n",
    "    atom_asym_id = torch.repeat_interleave(\n",
    "        bb_asym_id.unsqueeze(2), num_atoms_per_res, dim=2\n",
    "    ).flatten(start_dim=1, end_dim=2)\n",
    "    dist_mat = torch.cdist(bb_pos, atom_pos)\n",
    "    inter_chain_mask = bb_asym_id.unsqueeze(-1) != atom_asym_id.unsqueeze(-2)\n",
    "    non_h_mask = torch.ones_like(inter_chain_mask)\n",
    "    interacting_res_mask = (\n",
    "        inter_chain_mask & non_h_mask & (dist_mat <= inter_chain_interact_dist_threshold)\n",
    "    )\n",
    "    complex_feats[\"inter_chain_interacting_idx\"] = torch.nonzero(\n",
    "        interacting_res_mask.squeeze(0), as_tuple=False\n",
    "    )[..., 0].unique()\n",
    "\n",
    "    try:\n",
    "        traj = md.load(file_path)\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Mdtraj failed to load file {file_path} with error {e}\")\n",
    "        traj = None\n",
    "    try:\n",
    "        pdb_ss = md.compute_dssp(traj, simplified=True) if traj is not None else None\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Mdtraj's call to DSSP failed with error {e}\")\n",
    "        pdb_ss = None\n",
    "    try:\n",
    "        pdb_rg = md.compute_rg(traj) if traj is not None else None\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Mdtraj's call to RG failed with error {e}\")\n",
    "        pdb_rg = None\n",
    "\n",
    "    metadata[\"coil_percent\"] = (\n",
    "        np.sum(pdb_ss == \"C\") / metadata[\"modeled_seq_len\"] if pdb_ss is not None else np.nan\n",
    "    )\n",
    "    metadata[\"helix_percent\"] = (\n",
    "        np.sum(pdb_ss == \"H\") / metadata[\"modeled_seq_len\"] if pdb_ss is not None else np.nan\n",
    "    )\n",
    "    metadata[\"strand_percent\"] = (\n",
    "        np.sum(pdb_ss == \"E\") / metadata[\"modeled_seq_len\"] if pdb_ss is not None else np.nan\n",
    "    )\n",
    "\n",
    "    metadata[\"radius_gyration\"] = pdb_rg[0] if pdb_rg is not None else np.nan\n",
    "\n",
    "    # Write features to pickles\n",
    "    utils.write_pkl(processed_path, complex_feats)\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def process_serially(all_paths, write_dir, skip_existing=False, verbose=False):\n",
    "    all_metadata = []\n",
    "    for file_path in tqdm(all_paths):\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            metadata = process_file(\n",
    "                file_path, write_dir, skip_existing=skip_existing, verbose=verbose\n",
    "            )\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Finished {file_path} in {elapsed_time:2.2f}s\")\n",
    "            if metadata is not None:\n",
    "                all_metadata.append(metadata)\n",
    "        except utils.DataError as e:\n",
    "            print(f\"Failed {file_path}: {e}\")\n",
    "    return all_metadata\n",
    "\n",
    "def process_fn(file_path, write_dir=None, skip_existing=False, verbose=False):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        metadata = process_file(file_path, write_dir, skip_existing=skip_existing, verbose=verbose)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        if verbose:\n",
    "            print(f\"Finished {file_path} in {elapsed_time:2.2f}s\")\n",
    "        return metadata\n",
    "    except utils.DataError as e:\n",
    "        if verbose:\n",
    "            print(f\"Failed {file_path}: {e}\")\n",
    "\n",
    "def main():\n",
    "    # Disable GPU\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "    \n",
    "    pdb_dir = CONFIG[\"pdb_dir\"]\n",
    "    all_file_paths = [\n",
    "        os.path.join(pdb_dir, item)\n",
    "        for item in os.listdir(pdb_dir)\n",
    "        if \".pdb\" in item\n",
    "    ]\n",
    "    total_num_paths = len(all_file_paths)\n",
    "    write_dir = CONFIG[\"write_dir\"]\n",
    "    os.makedirs(write_dir, exist_ok=True)\n",
    "    \n",
    "    metadata_file_name = \"rna_metadata_debug.csv\" if CONFIG[\"debug\"] else \"rna_metadata.csv\"\n",
    "    metadata_path = os.path.join(write_dir, metadata_file_name)\n",
    "    print(f\"Files will be written to {write_dir}\")\n",
    "\n",
    "    # Process each PDB file\n",
    "    if CONFIG[\"num_processes\"] == 1 or CONFIG[\"debug\"]:\n",
    "        all_metadata = process_serially(\n",
    "            all_file_paths, write_dir, skip_existing=CONFIG[\"skip_existing\"], verbose=CONFIG[\"verbose\"]\n",
    "        )\n",
    "    else:\n",
    "        _process_fn = fn.partial(\n",
    "            process_fn, write_dir=write_dir, skip_existing=CONFIG[\"skip_existing\"], verbose=CONFIG[\"verbose\"]\n",
    "        )\n",
    "        with mp.Pool(processes=CONFIG[\"num_processes\"]) as pool:\n",
    "            all_metadata = pool.map(_process_fn, all_file_paths)\n",
    "        all_metadata = [x for x in all_metadata if x is not None]\n",
    "    \n",
    "    metadata_df = pd.DataFrame(all_metadata)\n",
    "    metadata_df.to_csv(metadata_path, index=False)\n",
    "    succeeded = len(all_metadata)\n",
    "    print(f\"Finished processing {succeeded}/{total_num_paths} files\")\n",
    "\n",
    "# Run the processing\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8630b2-28ea-466e-b039-ed612435bf12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "253cb2ea-7d77-47b0-8269-c6dd422d7003",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee36f3f-5420-458c-ab9c-a6608d60abc9",
   "metadata": {},
   "source": [
    "### train_se3_flows.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db530863-0acb-40bd-803c-2377423f5583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import GPUtil\n",
    "import torch\n",
    "\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from pytorch_lightning.trainer import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from src.data.pdb_na_datamodule_base import PDBNABaseDataModule\n",
    "from src.models.flow_module import FlowModule\n",
    "import src.utils as eu\n",
    "import wandb\n",
    "\n",
    "log = eu.get_pylogger(__name__)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "class Experiment:\n",
    "    def __init__(self, *, cfg: DictConfig):\n",
    "        self._cfg = cfg\n",
    "        self._data_cfg = cfg.data_cfg\n",
    "        self._exp_cfg = cfg.experiment\n",
    "        self._model = FlowModule(self._cfg)\n",
    "        self._datamodule = PDBNABaseDataModule(data_cfg=self._data_cfg)\n",
    " \n",
    "    def train(self):\n",
    "        callbacks = []\n",
    "        \n",
    "        if self._exp_cfg.debug:\n",
    "            log.info(\"Debug mode.\")\n",
    "            logger = None\n",
    "            self._exp_cfg.num_devices = 1\n",
    "            self._data_cfg.loader.num_workers = 0\n",
    "        else:\n",
    "            logger = WandbLogger(**self._exp_cfg.wandb,)\n",
    "            \n",
    "            # Checkpoint directory\n",
    "            ckpt_dir = self._exp_cfg.checkpointer.dirpath\n",
    "            os.makedirs(ckpt_dir, exist_ok=True)\n",
    "            log.info(f\"Checkpoints saved to {ckpt_dir}\")\n",
    "            \n",
    "            # Model checkpoints\n",
    "            callbacks.append(ModelCheckpoint(**self._exp_cfg.checkpointer))\n",
    "            \n",
    "            # Save config\n",
    "            cfg_path = os.path.join(ckpt_dir, 'config.yaml')\n",
    "            with open(cfg_path, 'w') as f:\n",
    "                OmegaConf.save(config=self._cfg, f=f.name)\n",
    "            cfg_dict = OmegaConf.to_container(self._cfg, resolve=True)\n",
    "            flat_cfg = dict(eu.flatten_dict(cfg_dict))\n",
    "            if isinstance(logger.experiment.config, wandb.sdk.wandb_config.Config):\n",
    "                logger.experiment.config.update(flat_cfg)\n",
    "\n",
    "        devices = GPUtil.getAvailable(order='memory', limit = 8)[:self._exp_cfg.num_devices]\n",
    "        log.info(f\"Using devices: {devices}\")\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            **self._exp_cfg.trainer,\n",
    "            callbacks=callbacks,\n",
    "            logger=logger,\n",
    "            use_distributed_sampler=False,\n",
    "            enable_progress_bar=True,\n",
    "            enable_model_summary=True,\n",
    "            devices=devices,\n",
    "        )\n",
    "\n",
    "        trainer.fit(\n",
    "            model=self._model,\n",
    "            datamodule=self._datamodule,\n",
    "            ckpt_path=self._exp_cfg.warm_start\n",
    "        )\n",
    "from hydra.core.config_store import ConfigStore\n",
    "from hydra import initialize, compose\n",
    "\n",
    "def main():\n",
    "    # Initialize hydra\n",
    "    with initialize(version_base=None, config_path=\"./configs\"):\n",
    "        # Compose the configuration\n",
    "        cfg = compose(config_name=\"config\")\n",
    "        \n",
    "        if cfg.experiment.warm_start is not None and cfg.experiment.warm_start_cfg_override:\n",
    "            # Loads warm start config.\n",
    "            warm_start_cfg_path = os.path.join(os.path.dirname(cfg.experiment.warm_start), 'config.yaml')\n",
    "            warm_start_cfg = OmegaConf.load(warm_start_cfg_path)\n",
    "\n",
    "            # Warm start config may not have latest fields in the base config.\n",
    "            # Add these fields to the warm start config.\n",
    "            OmegaConf.set_struct(cfg.model, False)\n",
    "            OmegaConf.set_struct(warm_start_cfg.model, False)\n",
    "            cfg.model = OmegaConf.merge(cfg.model, warm_start_cfg.model)\n",
    "            OmegaConf.set_struct(cfg.model, True)\n",
    "            log.info(f'Loaded warm start config from {warm_start_cfg_path}')\n",
    "\n",
    "        exp = Experiment(cfg=cfg)\n",
    "        exp.train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9712427-e703-48bb-9dee-c0c37e9e84da",
   "metadata": {},
   "source": [
    "The training code executes successfully after Weights & Biases (wandb) authorization, but its runtime is significantly long, depending on the GPU used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85973ec8-e774-48aa-9a41-05b911c95da9",
   "metadata": {},
   "source": [
    "After training, the final saved checkpoint can be found at ckpt/se3-fm/rna-frameflow/last.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985c2789-9c5a-4371-b7eb-b048d96398b9",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "raw",
   "id": "cfa0f730-79b0-4734-af4e-4266c68618b7",
   "metadata": {},
   "source": [
    "# download model weights (https://drive.google.com/drive/folders/1umg0hgkBl7zsF_2GdCIKkfsRWbJNEOvp?usp=sharing)\n",
    "cd camera_ready_ckpts/\n",
    "gdown 1AnDMUa6ZnaRQonQje3Sfo1KBSQAsNKXe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6da6a82-a72f-489f-b0a6-fbded5eabb10",
   "metadata": {},
   "source": [
    "### Inference_se3_flows.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be802574-124b-4ea7-a200-c7aa142e4781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikhi\\anaconda3\\envs\\rna-bb-design\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\nikhi\\anaconda3\\envs\\rna-bb-design\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "c:\\Users\\nikhi\\anaconda3\\envs\\rna-bb-design\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Generating sequences with the following lengths: [40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150]\n",
      "Predicting DataLoader 0:   4%|▍         | 1/24 [00:07<02:45,  0.14it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikhi\\anaconda3\\envs\\rna-bb-design\\lib\\site-packages\\pytorch_lightning\\loops\\prediction_loop.py:257: predict returned None if it was on purpose, ignore this warning...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 24/24 [11:16<00:00,  0.04it/s]\n",
      "Starting EvalSuite on generated backbones ...\n",
      "Sample directory: cam_ready_rna_bb_samples/generated_samples/\n",
      "Created savedir: evalsuite_metrics\n",
      "Instantiating gRNAde v0.3\n",
      "    Using device: cpu\n",
      "    Creating RNA graph featurizer for max_num_conformers=1\n",
      "    Initialising GNN encoder-decoder model\n",
      "    Loading model checkpoint: src/tools/grnade_api/checkpoints/gRNAde_ARv1_1state_all.h5\n",
      "Finished initialising gRNAde v0.3\n",
      "\n",
      "Flattening directory ...\n",
      "Loaded metadata CSV with 343 filtered cluster samples ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra import initialize, compose\n",
    "\n",
    "import src.utils as eu\n",
    "from src.models.flow_module import FlowModule\n",
    "from src.data.pdb_na_dataset_base import LengthDataset\n",
    "from src.analysis.evalsuite import EvalSuite\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "log = eu.get_pylogger(__name__)\n",
    "\n",
    "class Sampler:\n",
    "    def __init__(self, cfg: DictConfig):\n",
    "        \"\"\"Initialize sampler.\n",
    "\n",
    "        Args:\n",
    "            cfg: inference config.\n",
    "        \"\"\"\n",
    "        ckpt_path = cfg.inference.ckpt_path\n",
    "        ckpt_dir = os.path.dirname(ckpt_path)\n",
    "        ckpt_cfg = OmegaConf.load(os.path.join(ckpt_dir, 'config.yaml'))\n",
    "\n",
    "        # Set-up config.\n",
    "        OmegaConf.set_struct(cfg, False)\n",
    "        OmegaConf.set_struct(ckpt_cfg, False)\n",
    "        cfg = OmegaConf.merge(cfg, ckpt_cfg)\n",
    "        cfg.experiment.checkpointer.dirpath = './'\n",
    "\n",
    "        self._cfg = cfg\n",
    "        self._infer_cfg = cfg.inference\n",
    "        self._samples_cfg = self._infer_cfg.samples\n",
    "        self._rng = np.random.default_rng(self._infer_cfg.seed)\n",
    "\n",
    "        # Set-up directories to write results to\n",
    "        self._ckpt_name = '/'.join(ckpt_path.replace('.ckpt', '').split('/')[-3:])\n",
    "        self._output_dir = os.path.join(\n",
    "            self._infer_cfg.output_dir,\n",
    "            self._infer_cfg.name,\n",
    "        )\n",
    "        os.makedirs(self._output_dir, exist_ok=True)\n",
    "        log.info(f'Saving results to {self._output_dir}')\n",
    "        config_path = os.path.join(self._output_dir, 'config.yaml')\n",
    "        with open(config_path, 'w') as f:\n",
    "            OmegaConf.save(config=self._cfg, f=f)\n",
    "        log.info(f'Saving inference config to {config_path}')\n",
    "\n",
    "        # Read checkpoint and initialize module.\n",
    "        self._flow_module = FlowModule.load_from_checkpoint(checkpoint_path=ckpt_path)\n",
    "        \n",
    "        self._flow_module.eval()\n",
    "        self._flow_module._infer_cfg = self._infer_cfg\n",
    "        self._flow_module._samples_cfg = self._samples_cfg\n",
    "        self._flow_module._output_dir = self._output_dir\n",
    "\n",
    "    def run_sampling(self):\n",
    "        log.info(\"Running on CPU\")\n",
    "        \n",
    "        eval_dataset = LengthDataset(self._samples_cfg)\n",
    "        dataloader = torch.utils.data.DataLoader(eval_dataset, batch_size=1, shuffle=False, drop_last=False)\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            accelerator=\"cpu\"\n",
    "        )\n",
    "\n",
    "        start_time = time.time()\n",
    "        trainer.predict(self._flow_module, dataloaders=dataloader)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        log.info(f'Finished in {elapsed_time:.2f}s')\n",
    "        log.info(f'Generated samples are stored here: {self._cfg.inference.output_dir}/{self._cfg.inference.name}/')\n",
    "\n",
    "def run():\n",
    "    # Initialize hydra\n",
    "    with initialize(version_base=None, config_path=\"./camera_ready_ckpts\"):\n",
    "        # Compose the configuration\n",
    "        cfg = compose(config_name=\"inference\")\n",
    "        \n",
    "        # Read model checkpoint and run inference\n",
    "        if cfg.inference.run_inference:\n",
    "            log.info('Starting inference on CPU')\n",
    "            sampler = Sampler(cfg)\n",
    "            sampler.run_sampling()\n",
    "\n",
    "        # Run optional eval\n",
    "        if cfg.inference.evalsuite.run_eval:\n",
    "            print(\"Starting EvalSuite on generated backbones ...\")\n",
    "            print(f\"Sample directory: {cfg.inference.output_dir}/{cfg.inference.name}/\")\n",
    "\n",
    "            rna_bb_samples_dir = f\"{cfg.inference.output_dir}/{cfg.inference.name}\"\n",
    "            saving_dir = cfg.inference.evalsuite.eval_save_dir\n",
    "            \n",
    "            # init evaluation module\n",
    "            evalsuite = EvalSuite(\n",
    "                        save_dir=saving_dir,\n",
    "                        paths=cfg.inference.evalsuite.paths,\n",
    "                        constants=cfg.inference.evalsuite.constants,\n",
    "                        gpu_id1=None,  # No GPU for inverse-folding model\n",
    "                        gpu_id2=None,  # No GPU for forward-folding model\n",
    "                    )\n",
    "            \n",
    "            # run self-consistency pipeline\n",
    "            metric_dict = evalsuite.perform_eval(\n",
    "                                    rna_bb_samples_dir,\n",
    "                                    flatten_dir=True\n",
    "                                )\n",
    "\n",
    "            # print out global self-consistency metrics\n",
    "            metrics_fp = os.path.join(saving_dir, \"final_metrics.pt\")\n",
    "            metric_dict = evalsuite.load_from_metric_dict(metrics_fp)\n",
    "            evalsuite.print_metrics(metric_dict) # print eval metrics\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72af604a-b470-44df-a597-f288a717bfa4",
   "metadata": {},
   "source": [
    "Results will be saved in the \"results\" folder.  However, evaluation requires metrics calculation, which necessitates instantiating the gRNAde model.  Since the gRNAde model functions on Linux, this code will run without issues in a Linux environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650d2a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
